{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-sequence-to-sequence-with-attention-mechanisms.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOL6rSdmb7+2f7kckPUij3k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/dive-to-deep-learning/blob/main/10-attention-mechanisms/2_sequence_to_sequence_with_attention_mechanisms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAs5q49ZmbM-"
      },
      "source": [
        "# Sequence to Sequence with Attention Mechanisms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt1ubR7kmrdw"
      },
      "source": [
        "In this notebook, we add the attention mechanism to the sequence to sequence (seq2seq) model to explicitly aggregate states with weights.It shows the model\r\n",
        "architecture for encoding and decoding at the time step $t$. \r\n",
        "\r\n",
        "Here, the memory of the attention layer consists of all the information that the encoder has seen—the encoder output at each time step. During the decoding, the decoder output from the previous time step $t - 1$ is used as the\r\n",
        "query. The output of the attention model is viewed as the context information, and such context is concatenated with the decoder input Dt. Finally, we feed the concatenation into the decoder.\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/seq2seq.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "To illustrate the overall architecture of seq2seq with attention model, the layer structure of its encoder and decoder is shown below.\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/seq2seq-layers.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A7w7VbkraE_"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfWieUnJsPxv"
      },
      "source": [
        "%%shell\r\n",
        "\r\n",
        "pip install -U mxnet-cu101==1.7.0\r\n",
        "pip install d2l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nsz_DbtqsEMD"
      },
      "source": [
        "from d2l import mxnet as d2l\r\n",
        "from mxnet import np, npx\r\n",
        "from mxnet.gluon import rnn, nn\r\n",
        "npx.set_np()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdXB7Y4-teAm"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DI-Paf0tey7"
      },
      "source": [
        "Technically speaking, the encoder transforms an input sequence of variable length into a fixedshape context variable c, and encodes the input sequence information in this context variable. We can use an RNN to design the encoder.\r\n",
        "\r\n",
        "Let us consider a sequence example (batch size: 1). Suppose that the input sequence is $x_1,..., x_T$ , such that $x_t$ is the tth token in the input text sequence. At time step $t$, the RNN transforms the input feature vector $X_t$ for $x_t$ and the hidden state $h_t-1$ from the previous time step into the current hidden state $h_t$. We can use a function $f$ to express the transformation of the RNNʼs recurrent layer:\r\n",
        "\r\n",
        "$$h_t = f(X_t, h_t - 1)$$\r\n",
        "\r\n",
        "In general, the encoder transforms the hidden states at all the time steps into the context variable through a customized function $q$:\r\n",
        "\r\n",
        "$$c = q(h_1, ..., h_T)$$\r\n",
        "\r\n",
        "For example, when choosing $q(h_1, ..., h_T ) = h_T$, the context variable is just the hidden state $h_T$ of the input sequence at the final time step.\r\n",
        "\r\n",
        "Now let us implement the RNN encoder. Note that we use an embedding layer to obtain the feature vector for each token in the input sequence. The weight of an embedding layer is a matrix whose number of rows equals to the size of the input vocabulary (vocab_size) and number of columns equals to the feature vectorʼs dimension (embed_size).\r\n",
        "\r\n",
        "For any input token index $i$, the embedding layer fetches the $i^{th}$ row (starting from 0) of the weight matrix to return its feature vector. Besides,\r\n",
        "here we choose a multilayer GRU to implement the encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIcpSzrExD_K"
      },
      "source": [
        "class Seq2SeqEncoder(d2l.Encoder):\r\n",
        "  \"\"\"The RNN encoder for sequence to sequence learning.\"\"\"\r\n",
        "\r\n",
        "  def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs):\r\n",
        "    super(Seq2SeqEncoder, self).__init__(**kwargs)\r\n",
        "\r\n",
        "    # Embedding layer\r\n",
        "    self.embedding = nn.Embedding(vocab_size, embed_size)\r\n",
        "    self.rnn = rnn.GRU(num_hiddens, num_layers, dropout=dropout)\r\n",
        "\r\n",
        "  def forward(self, X, *args):\r\n",
        "    # The output `X` shape: (`batch_size`, `num_steps`, `embed_size`)\r\n",
        "    X = self.embedding(X)\r\n",
        "    # In RNN models, the first axis corresponds to time steps\r\n",
        "    X = X.swapaxes(0, 1)\r\n",
        "    state = self.rnn.begin_state(batch_size=X.shape[1], ctx=X.ctx)\r\n",
        "    output, state = self.rnn(X, state)\r\n",
        "    # `output` shape: (`num_steps`, `batch_size`, `num_hiddens`)\r\n",
        "    # `state[0]` shape: (`num_layers`, `batch_size`, `num_hiddens`)\r\n",
        "    return output, state"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VujaSWAzy127"
      },
      "source": [
        "Let us still use a concrete example to illustrate the above encoder implementation. Below we instantiate a twolayer GRU encoder whose number of hidden units is 16. Given a minibatch of sequence inputs X (batch size: 4, number of time steps: 7), the hidden states of the last layer at all the time steps (output return by the encoderʼs recurrent layers) are a tensor of shape (number of time steps, batch size, number of hidden units)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gS2bZCNty-x-",
        "outputId": "e9973360-d689-4c54-92b2-2402d96cd85c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2)\r\n",
        "encoder.initialize()\r\n",
        "\r\n",
        "X = np.zeros((4, 7))\r\n",
        "output, state = encoder(X)\r\n",
        "output.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7, 4, 16)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_ASIWfizjaL"
      },
      "source": [
        "Since a GRU is employed here, the shape of the multilayer hidden states at the final time step is (number of hidden layers, batch size, number of hidden units). If an LSTM is used, memory cell information will also be contained in state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JqbAbvmzlWA",
        "outputId": "6766ab71-cb77-46c3-9216-6571c0735a58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(state), state[0].shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, (2, 4, 16))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5Rp2HMvsitH"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjGA-8XKsjxU"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20dvqyCCsGz8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}