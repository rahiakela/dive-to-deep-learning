{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-sequence-to-sequence-with-attention-mechanisms.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNVUDgxKXqlO++2v4jnBaQt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/dive-to-deep-learning/blob/main/10-attention-mechanisms/2_sequence_to_sequence_with_attention_mechanisms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAs5q49ZmbM-"
      },
      "source": [
        "# Sequence to Sequence with Attention Mechanisms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt1ubR7kmrdw"
      },
      "source": [
        "In this notebook, we add the attention mechanism to the sequence to sequence (seq2seq) model to explicitly aggregate states with weights.It shows the model\r\n",
        "architecture for encoding and decoding at the time step $t$. \r\n",
        "\r\n",
        "Here, the memory of the attention layer consists of all the information that the encoder has seenâ€”the encoder output at each time step. During the decoding, the decoder output from the previous time step $t - 1$ is used as the\r\n",
        "query. The output of the attention model is viewed as the context information, and such context is concatenated with the decoder input Dt. Finally, we feed the concatenation into the decoder.\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/seq2seq.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "To illustrate the overall architecture of seq2seq with attention model, the layer structure of its encoder and decoder is shown below.\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/seq2seq-layers.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A7w7VbkraE_"
      },
      "source": [
        ""
      ]
    }
  ]
}